{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3.session import Session\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: AWS CLI version 2, the latest major version of the AWS CLI, is now stable and recommended for general use. For more information, see the AWS CLI version 2 installation instructions at: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\n",
      "\n",
      "usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]\n",
      "To see help text, you can run:\n",
      "\n",
      "  aws help\n",
      "  aws <command> help\n",
      "  aws <command> <subcommand> help\n",
      "aws: error: argument operation: Invalid choice, valid choices are:\n",
      "\n",
      "get-role-credentials                     | list-account-roles                      \n",
      "list-accounts                            | logout                                  \n",
      "help                                    \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f06605b40d62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aws sso login --profile sandbox-administrator-access'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sandbox-administrator-access\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Session' is not defined"
     ]
    }
   ],
   "source": [
    "!aws sso login --profile sandbox-administrator-access\n",
    "session = Session(profile_name=\"sandbox-administrator-access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BUCKET = \"cognomotiv-mb-chai\"\n",
    "OUTPUT_BUCKET = \"cognomotiv-mb-pipeline-update\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load models and aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_keys(session, bucket):\n",
    "    s3_client = session.client(\"s3\")\n",
    "    next_token = None\n",
    "    while True:\n",
    "        if next_token is None:\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket)\n",
    "        else:\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket, ContinuationToken=next_token)\n",
    "        next_token = response.get('NextContinuationToken')\n",
    "        contents = response.get('Contents')\n",
    "        if contents is not None:\n",
    "            for content in contents:\n",
    "                if content['Key'][-4:] == '.zip':\n",
    "                    yield content['Key']\n",
    "                    # print(f\"s3://{bucket}/{content['Key']}\")\n",
    "                    # s3_client.download_file('MyBucket', 'hello-remote.txt', 'hello2.txt')\n",
    "        if next_token is None:\n",
    "            break\n",
    "\n",
    "def download_model(session, bucket, key, tmpdir='./tmp'):\n",
    "    if not os.path.isdir(tmpdir):\n",
    "        os.mkdir(tmpdir)\n",
    "    download_path = f\"{tmpdir}/model.zip\"\n",
    "    s3_client = session.client(\"s3\")\n",
    "    s3_client.download_file(bucket, key, download_path)\n",
    "    unzip_path = f\"{tmpdir}/saved_model\"\n",
    "    if not os.path.isdir(unzip_path):\n",
    "        os.mkdir(unzip_path)\n",
    "    with zipfile.ZipFile(download_path, 'r') as zf:\n",
    "        zf.extractall(unzip_path)\n",
    "    return unzip_path\n",
    "\n",
    "def wipe_tmpdir(tmpdir='./tmp'):\n",
    "    shutil.rmtree(tmpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://cognomotiv-mb-chai/00044bddd57c_1_0/VibClassifierTrainingInsightsModel.zip\n",
      "s3://cognomotiv-mb-chai/00044bddd57c_1_1/VibClassifierTrainingInsightsModel.zip\n",
      "INFO:tensorflow:Assets written to: aggregated/assets\n"
     ]
    }
   ],
   "source": [
    "weights = None\n",
    "nagg = 0\n",
    "for key in get_model_keys(session, INPUT_BUCKET):\n",
    "    print(f\"s3://{INPUT_BUCKET}/{key}\")\n",
    "    edge_uuid = key.split('/')[0]\n",
    "    unzip_path = download_model(session, INPUT_BUCKET, key)\n",
    "    model = load_model(unzip_path)\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    else:\n",
    "        weights = [x + y for x, y in zip(weights, model.get_weights())]\n",
    "    nagg += 1\n",
    "    wipe_tmpdir()\n",
    "weights = [x / nagg for x in weights]\n",
    "\n",
    "# load base model\n",
    "model = load_model(\"./notebooks/CHAI/model_chai_larger\")\n",
    "model.set_weights(weights)\n",
    "model.save(\"aggregated\")\n",
    "\n",
    "# TODO: convert to edge trainable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update edge CIM package and upload to s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_CIM_BASE = \"edge/mb-nvh-cim\"\n",
    "EDGE_CIM_MODEL_DIR = f\"{EDGE_CIM_BASE}/model\"\n",
    "EDGE_CIM_PLUGIN_DESCRIPTOR = f\"{EDGE_CIM_BASE}/plugin.json\"\n",
    "\n",
    "def update_plugin_model():\n",
    "    if os.path.isdir(EDGE_CIM_MODEL_DIR):\n",
    "        shutil.rmtree(EDGE_CIM_MODEL_DIR)\n",
    "    shutil.copytree(\"./aggregated\", EDGE_CIM_MODEL_DIR)\n",
    "\n",
    "def increment_plugin_version():\n",
    "    with open(EDGE_CIM_PLUGIN_DESCRIPTOR, 'r') as f:\n",
    "        plugin_json = json.load(f)\n",
    "    plugin_version = int(plugin_json['plugin_version'])\n",
    "    plugin_version += 1\n",
    "    plugin_json['plugin_version'] = str(plugin_version)\n",
    "    with open(EDGE_CIM_PLUGIN_DESCRIPTOR, 'w') as f:\n",
    "        json.dump(plugin_json, f, indent=2)\n",
    "    return plugin_version\n",
    "\n",
    "def upload_plugin(package_file, bucket):\n",
    "    s3_client = session.client(\"s3\")\n",
    "    s3_client.upload_file(package_file , bucket , f\"plugin/insights/{package_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/variables/variables.index\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/variables/variables.data-00000-of-00001\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/saved_model.pb\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/keras_metadata.pb\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/scaler/scaler.txt\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/plugin.json\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/mb-nvh-cim.so\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/.ipynb_checkpoints/plugin-checkpoint.json\n",
      "MD5: 01afcf9aa6040cfb35e3fa89badf6c78 (782252)\n"
     ]
    }
   ],
   "source": [
    "update_model()\n",
    "new_version = increment_version()\n",
    "print(new_version)\n",
    "package_file = f\"mb-nvh-cim.{new_version}.zip\"\n",
    "!cd /data/notebooks/takumi/CHAI/{EDGE_CIM_BASE} && /data/notebooks/takumi/CHAI/cognomotiv-plugin-packager -v {new_version} -p /data/notebooks/takumi/CHAI/{package_file} /data/notebooks/takumi/CHAI/{EDGE_CIM_BASE}\n",
    "upload_plugin(package_file, OUTPUT_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/variables/variables.index\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/variables/variables.data-00000-of-00001\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/saved_model.pb\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/keras_metadata.pb\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/scaler/scaler.txt\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/plugin.json\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/mb-nvh-cim.so\n",
      "/data/notebooks/takumi/CHAI/edge/mb-nvh-cim/.ipynb_checkpoints/plugin-checkpoint.json\n",
      "MD5: cb68a922401ba383790af597d174928f (782924)\n"
     ]
    }
   ],
   "source": [
    "!cd /data/notebooks/takumi/CHAI/{EDGE_CIM_BASE} && /data/notebooks/takumi/CHAI/cognomotiv-plugin-packager -v {new_version} -p /data/notebooks/takumi/CHAI/{package_file} /data/notebooks/takumi/CHAI/{EDGE_CIM_BASE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  mb-nvh-cim.3.zip\n",
      "  Length      Date    Time    Name\n",
      "---------  ---------- -----   ----\n",
      "     1686  1980-01-00 00:00   ata/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/variables/variables.index\n",
      "   180442  1980-01-00 00:00   ata/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/variables/variables.data-00000-of-00001\n",
      "   373349  1980-01-00 00:00   ata/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/saved_model.pb\n",
      "    25065  1980-01-00 00:00   ata/notebooks/takumi/CHAI/edge/mb-nvh-cim/model/keras_metadata.pb\n",
      "      205  1980-01-00 00:00   ata/notebooks/takumi/CHAI/edge/mb-nvh-cim/scaler/scaler.txt\n",
      "      261  1980-01-00 00:00   ata/notebooks/takumi/CHAI/edge/mb-nvh-cim/plugin.json\n",
      "  2231960  1980-01-00 00:00   ata/notebooks/takumi/CHAI/edge/mb-nvh-cim/mb-nvh-cim.so\n",
      "      261  1980-01-00 00:00   ata/notebooks/takumi/CHAI/edge/mb-nvh-cim/.ipynb_checkpoints/plugin-checkpoint.json\n",
      "---------                     -------\n",
      "  2813229                     8 files\n"
     ]
    }
   ],
   "source": [
    "!unzip -l mb-nvh-cim.3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update pipeline config and upload to s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_PIPELINE_DESCRIPTOR = \"edge/pipeline.json\"\n",
    "\n",
    "def set_pipeline_config_cim_version(plugin_name, version):\n",
    "    with open(EDGE_PIPELINE_DESCRIPTOR, 'r') as f:\n",
    "        pipeline_json = json.load(f)\n",
    "    # print(pipeline_json)\n",
    "    for si, stream in enumerate(pipeline_json['streams']):\n",
    "        insights = stream.get('insights')\n",
    "        if insights is None:\n",
    "            continue\n",
    "        for ii, insight in enumerate(insights):\n",
    "            if insight.get(\"plugin_name\") == plugin_name:\n",
    "                pipeline_json['streams'][si]['insights'][ii][\"plugin_version\"] = str(version) \n",
    "    with open(EDGE_PIPELINE_DESCRIPTOR, 'w') as f:\n",
    "        json.dump(pipeline_json, f, indent=2)\n",
    "\n",
    "def upload_pipeline_config(bucket):\n",
    "    s3_client = session.client(\"s3\")\n",
    "    s3_client.upload_file(EDGE_PIPELINE_DESCRIPTOR , bucket , f\"conf/{EDGE_PIPELINE_DESCRIPTOR.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_pipeline_config_cim_version(\"mb-nvh-cim\", new_version)\n",
    "upload_pipeline_config(OUTPUT_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
